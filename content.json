[{"title":"","date":"2017-07-26T15:08:00.000Z","path":"2017/07/26/机器学习算法（二）逻辑斯蒂回归/","text":"上文中说过，逻辑斯蒂回归虽然称为回归，但它实际上是一种分类算法。认识逻辑斯蒂回归，首先需要知道sigmoid函数。下面公式1即为sigmoid函数$$g\\left( x\\right) =\\dfrac {1}{1+e^{-x}}$$它的函数图像如图所示。 ##1、算法介绍和上文中的回归算法一样，我们有m条数据，每条数据有n个特征和1个标签。不同的是，上文的标签是一个连续型变量，本文中的标签是一个离散型变量，且它只有两个值，[0,1]。在线性回归中，有$\\widehat{y}=\\theta ^{T}x$，而在逻辑斯蒂回归中，其结果就是在线性回归外面套上sigmoid函数，即$$\\widehat {y}=\\dfrac {1}{1+e^{-\\theta ^{T}x}}$$因此，$\\widehat {y}$是一个连续变量，其取值范围为(0,1)。 对于$y$和$\\widehat {y}$,我们希望，当$\\widehat {y}$接近1时，$y$也大概率为1；当$\\widehat {y}$接近0时，$y$也大概率为0。因此可以做出如下假设。$$p(y=1| x,\\theta )=\\widehat {y}$$ $$p(y=0| x,\\theta )=1-\\widehat {y}$$以上两式可以统一为$$P\\left( y| x,\\theta \\right) =\\widehat {y}^{y}\\left( 1-\\widehat {y}\\right) ^{1-y}$$同理，由于x和y均已知，它是一个关于$\\theta$的函数。据此可求出其最大似然函数：$$L\\left( \\theta \\right) =\\prod ^{m}{i=1}\\widehat {y}^{y}\\left( 1-\\widehat {y}\\right) ^{1-y}$$，$i$为第$i$个样本。同理，可求其对数似然函数为$l\\left( \\theta \\right)$，并对每个$\\theta$求偏导。求导结果为：$$\\dfrac {\\partial l\\left( \\theta \\right) }{\\partial \\theta{ j}}=\\sum ^{m}{i=1}\\left( y^{i}-\\widehat {y}^{i}\\right) x^{i}{j}$$ 对其进行梯度上升学习，有$$\\theta {j}=\\theta {j}+\\alpha \\left( y-\\widehat {y}\\right) x_{j}$$ 逻辑斯蒂回归的目标函数被习惯性的认为成对数似然函数的相反数，即$$loss=-l\\left( \\theta \\right) $$ 前文中，我们认为y的取值为0，1。若认为y的取值为+1和-1，则可推导出一个较为优美的损失函数如下。$$loss=\\sum ^{m}{i=1}\\left[ \\ln \\left( 1+e^{-y{i}\\widehat {y}_{i}}\\right) \\right] $$","tags":[]},{"title":"","date":"2017-07-26T15:02:50.000Z","path":"2017/07/26/自然语言处理（六）词向量/","text":"目的：把文本用数据的形式表达出来方法：传统基于规则，现代基于统计 ###一、词编码方式1——离散表示1、One-hot编码 和句子中顺序无关，耗空间耗时 2、词袋模型每个数表示该词出现的次数（One-hot的加和） 3、TF_IDF每个数代表该词在整个文档中的占比 4、N-gram相邻N个词作为一组进行编码，缺点是浪费空间、无法衡量词之间的关系 ###二、词编码方式2——分布式表示 所谓分布式表示，就是将“红色小型汽车”变成“红色+小型+汽车” 于是产生了现代统计自然语言处理中最有创见的想法之一：用一个词附近的其他词来表示该词。 一开始用“共现矩阵”表示，后来发现它有耗空间过大、稀疏等问题，因此需要找到方法构造低维稠密向量作为词的分布式表示 (25~1000维)! 一开始有用SVD分解的方法，后来发现它与其他深度学习模型框架差异大 又有了NNLM方法如下图所示：（七月算法自然语言处理课程PPT） 这个模型复杂度较高。于是有了现在最常用的word2vec模型。 ###三、Work2Vec ####1、CBOW待续 ####2、Skip-Gram待续 当语料特别大时，多用Skip-Gram -》Word2Vec对多义词表示的并不好，因此有了改进版本GloVe，但用的不多。 -》还有sense2vec模型，它是将词性和word2vec结合的技术。 -》seq2seq 待续","tags":[]},{"title":"","date":"2017-07-26T15:02:41.000Z","path":"2017/07/26/自然语言处理（五）深度学习/","text":"###1、tips1、行业基准：用词袋模型表示句子，用SVM或LR做回归，用自己的模型和它做对比2、分词：启发式或机器学习（HMM，CRF）3、深度学习是端到端的 ###2、Auto-Encoder可将语料编码化，降维降噪 ###3、CNN机器自动学习卷积滤镜 用word2vec将一句话处理成一个矩阵，用CNN 案例：文本—&gt;(预处理、TF-IDF、word2vec) —&gt;词向量—&gt;(LR、SVM、LSTM)—&gt;标签 ###4、RNN—&gt;LSTM通过遗忘门、记忆门等控制。例如，异或就是遗忘，和1相与就是记忆等。 遗忘还是记忆是机器自己学习的。 另外还有更新门和输出门 案例：用于判断生成下一个词、句子、单词、图片等。 ####总结：AE用于压缩问题；CNN用于文本分类；RNN用于文本生成","tags":[]},{"title":"","date":"2017-07-26T15:02:27.000Z","path":"2017/07/26/自然语言处理（四）统计机器翻译SMT/","text":"###1、统计机器翻译三要素1、翻译模型2、语言模型3、排序模型 ###2、翻译流程1、双语数据预处理2、词对齐3、构造短语翻译表4、对短语翻译表进行概率估计5、解码，beam search6、评估","tags":[]},{"title":"","date":"2017-07-26T15:02:09.000Z","path":"2017/07/26/自然语言处理（三）主题模型/","text":"为了解决“一词多义”和“多词一意”的问题，引入“主题” LDA本质是一个三层贝叶斯网络 ###1、共轭分布1、Beta分布是二项分布的共轭先验分布2、Dirichlet分布是多项分布的共轭先验分布 Dirichlet分布的参数$[\\alpha{1},\\alpha{2},….,\\alpha_{n}]$，一般$\\alpha$都取一样的值。 $\\alpha=1\\quad$ 均匀分布 $\\alpha&gt;1\\quad$ 主题分布相等的概率增大 $\\alpha&lt;1\\quad$ 某一主题突出的概率增大 ###2、LDA模型LDA模型框图如下图所示 流程如下： 1、取$\\alpha$,用$Dir(\\alpha)$采样得到主题分布$v{m}$2、对$v{m}$采样得到某一主题3、取$\\beta$,用$Dir(\\beta)$对每个主题分别采样得到词分布$\\varphi_{k}$4、取得到的主题的对应的词分布，5、在词分布中采一个词 ###3、LDA问题 LDA问题的已知是：已知词语$w$(可观测),已知先验超参数$\\alpha$和$\\beta$ LDA问题的所求是：主题z，主题分布$\\theta$,词分布$\\varphi$ ###4、Gibbs采样Gibbs采样是一个为词语标记主题的算法。其流程如下：1、随机为文本中每个词分配主题2、统计：a.每个主题z下出现词t的数量. b.每个文档m下出现主题z的数量3、计算$P(z{i}|z{i-1,},w)$,返回步骤2，迭代。","tags":[]},{"title":"","date":"2017-07-26T15:02:03.000Z","path":"2017/07/26/自然语言处理（二）语言模型/","text":"##1、词袋模型认为词语间相互独立，失去词语间的顺序信息，相当于把词放在一个袋子里。 ##2、N-gram模型引入了词与词之间的顺序。 这个N是一个超参数。1、一般能用2-gram尽量用2-gram。2、平时3-gram用的多。3、n&gt;=4的情况很少，在有特别多语料时可以尝试到5-gram","tags":[]},{"title":"","date":"2017-07-26T15:01:58.000Z","path":"2017/07/26/自然语言处理（一）基础/","text":"##1、字符串常用命令 ##2、正则表达式 ##3、Python的re模块 ##4、jieba分词工具","tags":[]},{"title":"","date":"2017-07-26T15:01:52.000Z","path":"2017/07/26/机器学习算法（九）EM和贝叶斯网络的结合  HMM模型/","text":"##1、隐马尔可夫HMM模型一个隐马尔可夫模型可以表示为$$\\lambda={A,B,\\pi}$$具体就不说了，比较基本。 ##2、HMM模型的三个基本问题1、概率计算问题：给定$\\lambda$和观测序列${x{i}}$,求$P(x{i}| \\lambda)$。主要方法是前向计算法或后向计算法 2、学习算法问题：对于给定的一个观察值序列，调整参数λ，使得观察值出现的概率p(σ|λ)最大 a.有隐变量,有监督时：HMM b.有隐变量，无监督：Baum-Welch c.无隐变量：大数定理 3、预测算法：对于给定模型和观察值序列，求可能性最大的状态序列主要方法是Viterbi译码法","tags":[]},{"title":"","date":"2017-07-26T15:01:46.000Z","path":"2017/07/26/机器学习算法（八）贝叶斯算法族、朴素贝叶斯/","text":"##一、贝叶斯网络本文介绍贝叶斯网络。贝叶斯网络与前面的大多数算法有一些区别，它归属与贝叶斯学派，属于判别式模型。前面介绍大多数算法归属于频率学派，属于生成式模型。 贝叶斯网络可以看成是一个DAG(有向无环图)模型 贝叶斯网络的三个知识点1、网络如图所示：$$A\\leftarrow C\\rightarrow B$$则在C给定的条件下，A与B独立。 2、网络如图所示：$$A\\rightarrow C\\rightarrow B$$C给定的条件下，A与B独立 3、网络如图所示：$$A\\rightarrow C\\leftarrow B$$C未知的条件下，A与B独立 ##二、朴素贝叶斯前提：假设各特征相互独立（条件独立），且重要性相同 一个样本点属于某一类的概率是$$P(y|x{1},…,x{n})=\\dfrac{P(y)P(x{1},…,x{n}|y)}{P(x{1},…,x{n})}=\\dfrac{P(y)\\prod^{n}{i=1}P(x{i}|y)}{P(x{1},…,x{n})}$$在朴素贝叶斯中，判别一个样本点属于那个类别的公式为$$y=argmax{y}P(y)\\prod^{n}{i=1}P(x_{i}|y)$$ 其中，这个$P(x_{i}|y)$当是离散时是多项式分布，是连续时是高斯分布。","tags":[]},{"title":"","date":"2017-07-26T15:01:39.000Z","path":"2017/07/26/机器学习算法（七）EM算法族  EM、GMM/","text":"#一、GMM算法EM算法实在是难以介绍清楚，因此我们用EM算法的一个特例GMM算法作为引入。 ##1、GMM算法问题描述GMM模型称为混合高斯分布，顾名思义，它是由几组分别符合不同参数的高斯分布的数据混合而成的。 假设有n个样本点$x{1},x{2},…,x_{n}$,它们来自K个不同的高斯分布。有如下参数： 1、不同高斯分布的数据占比：$\\pi{i}$2、每个高斯分布的均值与方差：$\\pi{i}~N(\\mu{i},\\sigma{i}^2)$ 我们的目的是求出每个$\\pi{i}$，$\\mu{i}$，$\\sigma_{i}$ 因此我们的目标即是求合适的$\\pi{i}$，$\\mu{i}$，$\\sigma_{i}$来最大化对数似然函数。 $$l{\\pi,\\mu\\sigma}(x)=\\sum^{N}{i=1}log[\\sum^{K}{k=1}\\pi{k}N(x{I}|\\mu{k},\\sigma_{k})]$$这个目标函数中既有对数又有加和，因此不能直接求导因此我们采用迭代的方法。 ##2、GMM迭代方法描述Step1:对于每一个样本点i，计算它由不同组分(第k个组分)生成的概率$$r(i,k)=\\dfrac{\\pi{k}N(x{i}|\\mu{k},\\sigma{k})}{\\sum^{K}{j=1}\\pi{j}N(x{i}|\\mu{j},\\sigma{j})}$$Step2:由各个样本点的$r(i,k)$更新参数$\\pi{i}$，$\\mu{i}$，$\\sigma{I}$ Step3:回到Step1，迭代更新 这其实就是EM算法的E步和M步的过程。 下面给出通用的EM算法伪代码。 ##3、EM算法 Repeat util 收敛{ (E步)：对每个样本$x{i}$,计算$$Q{i}(z^{i})=P(z^{i}|x^{i};\\theta)$$ (M步)：对每个参数$\\theta$，有$$\\theta:=argmax{\\theta }l\\left( \\theta \\right) =argmax{\\theta}\\sum^{}{i}\\sum^{}{z^{i}}Q{i}(z^{i})log\\dfrac{P(x{i};z{i};\\theta)}{Q{i}(z_{i})}$$} 其中，E步的那个$Q$就是第i个样本的分布，就是那个$r(i,k)$这个形式可以推导可得，其实是等价的 M步中，那个公式就是对数似然函数，求使它最大化的参数 总结：EM算法说到底是一个迭代更新的过程。它首先对各个样本计算分布，然后更新参数；再计算分布，再更新参数……","tags":[]},{"title":"","date":"2017-07-26T15:01:32.000Z","path":"2017/07/26/机器学习算法（六）K-Means聚类、层次聚类、密度聚类、谱聚类/","text":"本文主要简述聚类算法族。聚类算法与前面文章的算法不同，它们属于非监督学习。 ##1、K-means聚类 记k个簇中心，为$\\mu{1}$,$\\mu{2}$,…,$\\mu{k}$,每个簇的样本数为$N{i}$假设每个簇中的数据都满足分布$N(\\mu{i},\\sigma)$，即方差相同，均值不同的GMM。则每一个样本点的分布函数为：$$\\phi{i}=\\dfrac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\dfrac{({x{i}-\\mu})^2}{2\\sigma^2})$$可求出其似然函数$$L{\\mu}=\\phi{1}\\times\\phi{2}\\times…$$且可求其对数似然为(以三个点为例)$$l{\\mu}=\\dfrac{1}{2}\\sum^{k}{j=1}\\sum^{i=1}{N{j}}(x{I}-\\mu{j})^2$$求驻点有：$$\\mu{j}=\\dfrac{1}{N{j}}\\sum^{N}{i=1}x{i}$$因此，以均方误差为目标函数的时候肯定是收敛的。用其他函数作为目标函数不一定收敛。 注：$k$的选择采用“手肘法”，注意不是交叉验证，它连标签都没有！ ##2、密度聚类代表算法：DBSCAN K-means对噪声敏感，密度聚类对噪声不敏感。 ##3、层次聚类 按层次聚类，由上至下或由下至上，优点是可以任意选择聚类数 ##4、谱聚类Step1:对样本点俩俩计算相似度$S{ij}$,组成相似度矩阵，又称权值矩阵$$W{n\\times n}=[S_{ij}]$$ Step2:将$W{n\\times n}$的主对角线元素全部置为0，把每行元素的值相加，第$i$行的和为$d{i}$。将$d{i}$作为主对角线元素组成$D{n\\times n}$ Step3:令$$L{n\\times n}=D{n\\times n}-W_{n\\times n}$$，称为拉普拉斯矩阵。这个$L$是半正定的，它最小的特征值为0。 Step4:求L矩阵的特征值和特征向量，将所有特征值从小大排列，取出前k个（聚类数为k），将其对应的特征向量如下排列：$$[u{1},u{2},…,u{k}]$$该矩阵的第一行即为第一个样本点转换后的特征，第二行为第二个样本点转换后的特征。将这些特征扔入K_means，其聚类结果即是谱聚类结果。","tags":[]},{"title":"","date":"2017-07-26T15:01:24.000Z","path":"2017/07/26/机器学习算法（五）集成/","text":"##1、集成集成指用多个基学习器共同构成一个更加强大的学习器。集成包含三种方法：Boosting，Bagging，Stacking1、Boosting:包括GBDT和Adaboost，各学习器间存在强依赖关系，只能串行实现2、Bagging的代表算法是随机森林，各学习器间不存在强依赖关系，可以并行实现3、Stacking主要是分层结构。每个初级学习器负责一部分的领域知识并构成次级学习器。 ##2、Boosting1、GBDT：梯度提升决策树原理简介如下图所示： 它的基本思想是：迭代第m次的基函数是根据上一颗树的伪残差训练出来的。 2、Adaboost输入：训练数据集$D={(x{1},y{1}),(x{2},y{2}),…,(x{N},y{N})}$初始化：为每条数据赋一个权重，通常$D{w}={w{11},w{12},…,w{1N}}$,其中，1表示第1次迭代。步骤：Step1:找到一个二分类器$G{m}(x)$,使下式的这个损失最小：$$e{m}=\\sum^{N}{i=1}w{mi}I(G{m}(x{I})\\neq y{i})$$Step2:计算$$a{m}=\\dfrac{1}{2}log\\dfrac{1-e{m}}{e{m}}$$Step3:更新权值$$w{m+1,i}=\\dfrac{w{mi}}{z{m}}exp(-\\alpha{m}y{i}G{m}(x{i}))$$其中，$z{m}$为一归一化因子Step4: $m=m+1$,并返回Step1进行迭代。 最终求得的分类函数为$$f(x)=\\sum^{N}{m=1}\\alpha{m}G_{m}(x)$$ ##3、Bagging比较有代表性的Bagging算法就是随机森林，它就是随机采一些样本，随机采一些属性生成多棵决策树进行共同决策。 Bagging可以减少方差Boosting可以减少偏差","tags":[]},{"title":"","date":"2017-07-26T15:01:16.000Z","path":"2017/07/26/机器学习算法（四）决策树/","text":"##一、信息熵首先给出信息熵的定义如下$$H\\left( x\\right) =-\\sum _{x\\in \\chi }p\\left( x\\right) \\ln p\\left( x\\right) $$1、无约束条件时，均匀分布熵最大2、若给定分布的期望和方差，则正态分布的熵最大 ##二、决策树是什么决策树就是下图所示的东西 ##三、决策树 ###1、几个名词：1、训练数据集：D2、数据的标签有K种，即有K个类，记为$C{k}$3、数据有多个特征，其中有某一个特征叫A，这个A特征有n个取值，记所有A特征取值为i的数据的集合为$D{i}$4、在子集$D{i}$中属于第k个类的样本集合记为$D{ik}$定义如下两个量： $$H\\left( D\\right) =\\sum ^{K}{k=1}\\dfrac {\\left| C{k}\\right| }{\\left| D\\right| }\\log\\dfrac {\\left| C{k}\\right| }{\\left| D\\right| }$$ $$H\\left( D| A\\right) =-\\sum ^{n}{i=1}\\dfrac {\\left| Di\\right| }{\\left| D\\right| }\\sum ^{K}{k=1}\\dfrac {\\left| D{ik}\\right| }{\\left| D{i}\\right| }log\\dfrac {\\left| D{ik}\\right| }{\\left| D_{i}\\right|}$$ ###2、评估指标根据以上定义的量，定义如下几个评估指标：1、信息增益：$g(D,A)=H(D)-H(D|A)$2、信息增益率：$g{r}(D,A)=g(D,A)/H(A)$3、基尼系数：$Gini(p)=1-\\sum ^{K}{k=1}(\\dfrac {\\left| C_{k}\\right| }{\\left| D\\right| })^{2}$ ###3、决策树算法常用决策树算法包括ID3算法、C4.5算法，CART决策树，它们最重要的不同在于评估指标不同，其中，ID3采用信息增益作为评估指标，C4.5采用信息增益率作为评估指标，CART决策树采用基尼系数作为评估指标。 我们以ID3为例，它首先扫描所有特征，找出信息增益最大的特征作为其根节点，在对其各个子节点递归地进行这个过程，直至达到某个收敛条件。 ###4、决策树的目标函数决策树的目标函数，或者说决策树的损失函数为：$C(T)=\\sum{t\\in leaf}N{t}\\times H(t)$其中，$N_{t}$代表某一叶结点中包含的样本数；$H(t)$代表该叶结点中的熵 对该目标函数进行正则化后的目标函数为：$C_{\\alpha}(T)=C(T)+\\alpha\\times|leafs|$,即加上叶节点个数的信息。","tags":[]},{"title":"","date":"2017-07-26T15:01:08.000Z","path":"2017/07/26/机器学习算法（三）支持向量机/","text":"##1、问题介绍本文只涉及二分类支持向量机。 支持向量机问题可以分为三种情况来讨论：1、硬间隔支持向量机：用于可以被一个超平面严格分开的问题中，又称为线性可分支持向量机2、软间隔支持向量机：用于可以被一个超平面非严格分开的问题中，又称线性支持向量机3、核支持向量机：用于可以被一个超曲面分开的问题中，又称非线性支持向量机 本文主要介绍硬间隔支持向量机。 所谓“可以被一个超平面严格分开”，以三维空间数据为例，就是如下图情况： 即可以找到一个分离超平面，将正负数据点分开。 假设我们有数据D={(x1,y1),(x2,y2),…,(xn,yn)},则x代表空间中的数据点，y代表该点的标签，它有两个取值，+1和-1。 我们要做的事就是找到一个如下分离超平面$$y\\left( x\\right) =\\omega ^{T}\\phi \\left( x\\right) +b$$这个分离超平面有如下两个特点：1、它可以将所有的正负例点分开2、在满足1的基础上，令所有点中，距离它距离最小的点的距离最大。 简单概括，就是找到一个分离超平面，使点到面的“最小距离最大化”。 我们的目标就是找到这个超平面的$\\omega$和$b$。 ##2、目标函数分析根据“最小距离最大化的”目标函数思想，可以写出支持向量机的目标函数如下式1：$$\\max \\left{ \\min {i}\\left[ \\dfrac {y{i}\\left( w^{T}\\phi \\left( x\\right) +b\\right) }{\\left| w\\right| }\\right] \\right} $$我们想要求的参数$w$与$b$可表述如下：$$w,b=argmax{w,b} \\left{ \\min {i}\\left[ \\dfrac {y_{i}\\left( w^{T}\\phi \\left( x\\right) +b\\right) }{\\left| w\\right| }\\right] \\right} $$ 对于目标函数，即公式1，我们总可以认为$$\\min {i}y{i}\\left( w^{T}\\phi \\left( x\\right) +b\\right) =1$$因此目标问题转化为： 求$w$和$b$,目标函数为 $$ \\max {w,b}\\dfrac {1}{\\left| w\\right| }$$进行整理，最终成为如下约束最优化问题$$\\min {w,b}\\dfrac {1}{2}\\left| w\\right| ^{2}$$ $$s.t. \\quad y{i}\\left( w^{T}\\phi \\left( x\\right) +b\\right) \\geq1$$对线性可分支持向量机而言，有$\\phi \\left( x{i}\\right) =x{i}$以下要用到约束最优化求解的知识。根据拉格朗日乘子法，可写出该规划问题的拉格朗日表达式：$$L\\left( w,b,\\alpha \\right) =\\dfrac {1}{2}\\left| w\\right| ^{2}-\\sum ^{n}{i=1}\\alpha {i}\\left( y{i}\\left( w^{T}\\phi \\left( x{i}\\right) +b\\right) -1\\right) $$,其中$$a{i}\\geq0$$因此有1、原问题：求$$\\min {w,b}\\dfrac {1}{2}\\left| w\\right| ^{2}$$ （$s.t. \\quad y{i}\\left( w^{T}\\phi \\left( x\\right) +b\\right) \\geq1$）2、转化为求$$\\min {w,b}\\max {\\alpha }L\\left( \\omega ,b,\\alpha \\right) $$3、根据拉格朗日对偶性，极小极大问题可以转化为极大极小问题。即转化为求公式2$$\\max {\\alpha }\\min {W,b}L\\left( \\omega ,b,\\alpha \\right)$$ 我们进行了一个如下过程的转换。（本文中$W$和$w$和$\\omega$都表示一个东西，手写软件不太给力）$$\\min {W,b}\\dfrac {1}{2}\\left| w\\right| ^{2}\\rightarrow \\min {w,b}\\max {\\alpha }L\\left( \\omega ,b,\\alpha \\right) \\rightarrow \\max {\\alpha }\\min _{W,b}L\\left( \\omega ,b,\\alpha \\right) $$ 我们根据$L\\left( \\omega ,b,\\alpha \\right)$写出方程式$\\dfrac {\\partial L}{\\partial \\omega }=0$和$\\dfrac {\\partial L}{\\partial b }=0$,可求出$\\omega$和$b$关于$\\alpha$的表达式，回代到公式2，可以整理成为如下约束规划问题。$$\\min {\\alpha }\\dfrac {1}{2}\\sum ^{n}{i=1}\\sum ^{n}{j=1}\\alpha {i}\\alpha {j}y{i}y{j}\\left( \\phi \\left( x{i}\\right) \\phi \\left( x{j}\\right) \\right) -\\sum ^{n}{i=1}\\alpha {i}$$$$S.t.\\sum ^{n}{i=1}\\alpha {i}y{i}=0,\\quad a_{i}\\geq0$$求出最优的$\\alpha$，就可以求出$\\omega$和$b$ ##3、线性支持向量机对于不能被严格分开的正负样本点，我们只能期望找到一个分离超平面，尽可能地把它分开。如下图可见，有些点是分错的，但我们允许这种错误。这种模型就是线性支持向量机，也称为软间隔支持向量机。仿照硬间隔支持向量机的格式，我们同样可以整理得到约束最优化问题如下：$$\\min {w,b,\\xi}\\dfrac {1}{2}\\left| w\\right| ^{2}+C\\sum ^{n}{i=1}\\xi $$ $$s.t. \\quad y{i}\\left( w^{T}\\phi \\left( x\\right) +b\\right) \\geq1-\\xi{i}\\quad\\xi{i}\\geq0$$同理可整理出来拉格朗日形式的约束最优化问题如下：$$\\min {\\alpha }\\dfrac {1}{2}\\sum ^{n}{i=1}\\sum ^{n}{j=1}\\alpha {i}\\alpha {j}y{i}y{j}\\left( \\phi \\left( x{i}\\right) \\phi \\left( x{j}\\right) \\right) -\\sum ^{n}{i=1}\\alpha {i}$$$$S.t.\\sum ^{n}{i=1}\\alpha {i}y{i}=0,\\quad0\\leq a{i}\\leq C$$对线性支持向量机而言，有$\\phi \\left( x{i}\\right) =x{i}$ ###两个小tips ####1、这个C的调参数：a.调小 过渡带变宽，可以防止过拟合b.调大 过渡带变窄，可以提高精度 ####2、损失函数对于分错的点，有一个损失$\\xi$(见上图),对于支持向量机来说，其损失函数为$$\\xi=\\xi{1}+\\xi{2}+…+\\xi_{n}$$该损失函数又称为“合页损失函数”。 ##4、核支持向量机以上两种模型的优化函数中，都有$\\phi \\left( x{i}\\right) =x{I}$，在核支持向量机中，有所不同。核支持向量机有不同的核，常用的是高斯核。核支持向量机和线性支持向量机的关系如下图：在高斯核中，有$$\\phi \\left( x{i}\\right) \\phi \\left( x{j}\\right) =exp\\left( -\\dfrac {\\left| x{i}-x{j}\\right| ^{2}}{2\\sigma ^{2}}\\right) $$","tags":[]},{"title":"","date":"2017-07-26T15:00:45.000Z","path":"2017/07/26/机器学习算法（一）线性回归/","text":"机器学习算法（一）线性回归本文主要梳理一下线性回归和逻辑斯蒂回归这两大算法。这两个算法的关系是什么呢？答案是并没有什么关系。这样说其实也不对，逻辑斯蒂回归里有线性回归的重要组成部分。但是二者又一个本质区别，就是线性回归是一个“回归”算法，而逻辑斯蒂回归是一个“分类”算法。这就导致两个算法永远像牛郎织女一样隔着银河。本文主要介绍线性回归，下篇文章会在本文的基础上介绍逻辑斯蒂回归。本文思路主要来自邹博的机器学习课程。本文为作者自己学习整理的笔记，转载请注明出处。 线性回归1、输入描述：有训练数据集D，其中共有m条数据，n个特征，如下表所示。 数据id 数学 语文 英语 物理 学生1 98 92 96 83 学生2 95 79 86 23 … 34 35 92 92 学生m 67 68 94 72 上表是一个数据表的简单示例，其中包含m条数据，每条数据代表了一名学生；4个特征，分别是数学成绩、语文成绩、英语成绩、物理成绩。 以上是一个具体的表格，我们可以抽象表格如下： 数据id 属性1 属性2 … 属性n 预测值 1 x11 x12 … x1n y1 2 x21 x22 … x2n y2 … … … … … … m xm1 xm2 … xmn ym 对于其中的某一条数据，可以认为如下： 属性1 属性2 … 属性n 预测值 x1 x2 … xn y 2、问题描述线性回归就是要解决以下问题： 要建立一如下公式1：$$y=h{\\theta }\\left( x\\right) =\\theta {0}+\\theta {1}x{1}+\\theta {2}x{2}+\\ldots +\\theta {n}x{n}$$ 我们的目标就是在已知一些数据后，求出该方程的系数，即$\\theta {0}$,$\\theta {1}$,…,$\\theta _{n}$. 3、算法推导上面公式1可做如下变形（公式2）$$\\widehat{y}=h{\\theta }\\left( x\\right) =\\sum ^{n}{i=1}\\theta {i}x{i}=\\theta ^{T}x$$ 对于数据标签的真实值，我们可以设为$y$,而对于数据用公式1和所有$x$计算得到的值为估计值，设为$\\widehat{y}$. 因此有公式3$$y=\\widehat {y}+\\varepsilon $$ 其中，这个$\\varepsilon$可以认为是噪声。 根据中心极限定理，我们可以认为这个$\\varepsilon$服从正态分布，即$$\\varepsilon \\sim N\\left( 0,\\sigma ^{2}\\right) $$因此，有公式4$$P\\left( \\varepsilon \\right) =\\dfrac {1}{\\sqrt {2\\pi \\sigma }}exp\\left( -\\dfrac {\\varepsilon ^{2}}{2\\sigma ^{2}}\\right) $$因为$$\\varepsilon=y-\\widehat {y}$$带入公式4，将$\\varepsilon$和$x$视为常数，则可将$P\\left( \\varepsilon \\right)$看成是关于$\\theta$的函数。即$P\\left( \\varepsilon \\right) =g\\left( \\theta \\right) $根据最大似然定理，可得关于$\\theta$的似然函数为公式5$$L\\left( \\theta \\right) =\\prod ^{m}_{i=1}\\dfrac {1}{\\sqrt {2\\pi }\\sigma }exp\\left( -\\dfrac {\\left( y^{i}-\\theta ^{T}x^{i}\\right) ^{2}}{2\\sigma ^{2}}\\right) $$其中，$i$表示第$i$条数据。该式中，仅$\\theta$为未知量，我们要做的事就是求出一组$\\theta$，使$L\\left( \\theta \\right)$最大。对于公式5，可以对其取$log$求出其对数似然函数。对其对数似然函数可以进一步化简，最终，我们可得到如下的目标函数（公式6）：求出一组$\\theta$使$$J\\left( \\theta \\right) =\\dfrac {1}{2}\\sum ^{m}_{i=1}\\left( \\theta ^{T}x^{i}-y^{i}\\right) ^{2}$$最小。此处略去求解过程，该式的解决方式是令$J\\left( \\theta \\right)$对$\\theta$求导并令其等于0。最后可求得公式7:$$\\theta =\\left( x^{T}x\\right) ^{-1}x^{T}y$$当以公式6为目标函数时，并没有考虑过拟合的问题。若想减少过拟合风险，可以对该目标函数加入正则项，常用的方法有以下两种：1、L1正则（又称LASSO回归） 其目标函数是公式8: $$J\\left( \\theta \\right) =\\dfrac {1}{2}\\sum ^{m}{i=1}\\left( \\theta ^{T}x^{i}-y^{i}\\right) ^{2}+\\lambda \\sum ^{n}{j=1}\\left| \\theta _{j}\\right| $$ 2、L2正则（又称岭回归）其目标函数是公式9: $$J\\left( \\theta \\right) =\\dfrac {1}{2}\\sum ^{m}{i=1}\\left( \\theta ^{T}x^{i}-y^{i}\\right) ^{2}+\\lambda \\sum ^{n}{j=1}\\theta ^{2}_{j}$$ 实际上，一般用公式7直接求出$\\theta$是很难的。一般我们会采用梯度下降的方法来求出一个局部最优的$\\theta$。 综上，线性回归的目标函数为：公式6、公式8或公式9。即$$loss=J\\left( \\theta \\right) =\\dfrac {1}{2}\\sum ^{m}{i=1}\\left( \\theta ^{T}x^{i}-y^{i}\\right) ^{2}$$或$$loss=J\\left( \\theta \\right) =\\dfrac {1}{2}\\sum ^{m}{i=1}\\left( \\theta ^{T}x^{i}-y^{i}\\right) ^{2}+\\lambda \\sum ^{n}{j=1}\\left| \\theta {j}\\right| $$或$$loss=J\\left( \\theta \\right) =\\dfrac {1}{2}\\sum ^{m}{i=1}\\left( \\theta ^{T}x^{i}-y^{i}\\right) ^{2}+\\lambda \\sum ^{n}{j=1}\\theta ^{2}_{j}$$","tags":[]},{"title":"","date":"2017-07-24T10:06:03.000Z","path":"2017/07/24/机器学习小白之路 （一）机器学习各项相关宏观知识综述/","text":"机器学习小白之路 （一）机器学习各项相关宏观知识综述大数据、人工智能、机器学习、数据挖掘、神经网络、深度学习等名词的释义及辨析在本文中，我主要分享一下我对机器学习一些宏观的看待，这些是我学习中的一点心得，不一定完全准确，如果有有问题的部分，欢迎大家进行批判指正。 1、人工智能、机器学习和神经网络的关系这三个词是依次包含的关系，具体方式为：人工智能的概念最大，它包含机器学习；机器学习的概念次之，它包含神经网络。人工智能是近几年兴起的概念，主要是指可以利用数据、及相关的一些算法技术，使机器产生对人的行为的模拟(个人理解，不一定准确)。人工智能技术包括机器学习、自然语言处理、语音识别、图像处理、模式识别等许多技术，它们互相之间既有连续又有区别。不过可以看到，机器学习不可能等同于人工智能，它只是人工智能的一部分，或者说是人工智能技术的一个重要理论工具。机器学习技术主要是指一系列的算法，这些算法可以通过对已知数据的分析和建模，对未知数据的某些行为作出指导。例如，我可以通过统计1000个男性和女性的身高，并对其分别建模。此后再给出一个人的身高体重数据，我可以通过已有的模型推测他大概是男性还是女性。机器学习包括许多技术，主要分为三个分支，即监督学习、非监督学习和增强学习。有学者曾经评出了“十大最经典的机器学习算法”，有兴趣的读者可以自行百度。机器学习包含多种算法，例如决策树、支持向量机、朴素贝叶斯等。而神经网络也是诸多机器学习算法中的一种。近几年，神经网络算法大火。然而，它并不是一个近期才发展起来的算法。很早就有了神经网络算法，它也曾经火过一段时间，然而，就像很多主播或网红一样，没过多久，它就过气了。它过气主要是因为当时它的未知参数过多导致计算量过大，而且它是一个“黑盒子”模型。因此，当时又出现了“人美声甜”的“支持向量机”算法，理论优美漂亮，参数可调可控，一举便占领了神经网络的宝座，抢走了神经网络的风头。然而风水轮流转，近些年也正是因为大数据技术的发展，计算能力得到了提高，由神经网络演变而来的深度学习技术成为了主导。深度学习技术也确实在图像、视觉、自然语言等方面发挥了它强大的实力。之前的AlphaGo模型就是采用了深度学习技术，让我们看到了机器智能化的潜力。因此神经网络又一举夺回当年的桂冠，成为了时下最炙手可热的算法。 2、机器学习与数据挖掘的关系要说机器学习和数据挖掘，其实他俩的区别真心不算特别大。很多书或教材也会把它们叫混，这倒也不是因为写东西的人不知道它们的区别，而实在是它们的区别不大，没必要那么强调它们的不同。要说起区分，机器学习比数据挖掘的概念要窄。我认为有如下公式。数据挖掘=机器学习+数据处理技术机器学习主要是一系列的算法理论，它是理论上的东西，强调各种算法的联系和区别、算法的优化等。而数据挖掘主要是强调工程上的东西，如如何获取到数据、如何存储这些数据、如何清洗这些数据、如何提取合适的特征、如何选用合适的机器学习算法及合适的参数、如何评估等。 3、神经网络与深度学习的关系深度学习是神经网络的一种，又称为“深度神经网络”。言外之意，神经网络还有“浅层的”，那种就只是神经网络，不是深度学习。神经网络的参数量巨大，深度学习更是动辄几百上千个参数，数据量的增多、计算能力的增强为深度学习开辟了土壤，在近几年大放异彩。 4、大数据与其他名词的关系我个人觉得，大数据这个词和前面的这些词并不是在描述同一个东西，或者它们看问题的切入点不一样。其他词语更在于“智能”和“挖掘”两个方面，而大数据的侧重点在它的“大”上。这个“大”可以是没有智能相关技术的大。例如，它可以仅是大的数据量、大的速度、大的存储，大的计算能力。但不可否认的是，正是因为大的数据，我们才能更好地从数据中挖掘信息，才有了今天的人工智能、深度学习等技术的飞黄腾达。我认为，大数据和人工智能更像是一个人的两条腿，是一个互相支持，互相促进的关系。","tags":[]},{"title":"","date":"2017-07-24T09:30:52.000Z","path":"2017/07/24/机器学习小白之路 （二）机器学习各项相关宏观知识综述/","text":"机器学习小白之路 （二）机器学习各项相关宏观知识综述各项技术的基本涵盖点和主要技术1、大数据技术前面已经多次提到了，大数据技术主要包括大数据的获取、存储、处理等。其相关核心技术包括：1、大数据的获取目前主要来源于大型互联网公司的数据、大量移动端数据、传感器数据等。同时大量的网页中包含许多非结构化数据，如淘宝图片、淘宝评论、微博评论等，这些数据需要通过爬虫获取。2、大数据的存储不同于一般数据的存储。一般数据可以存储在关系型数据库里，而大数据由于数量巨大，往往很难用一台机器的数据库进行存储，因此可以存储在分布式数据库里。另外，由于大量非结构化数据的存在，这些数据需要存储在非关系型数据库里。3、大数据的计算需要很强的计算能力，在早年只有CPU的时候，执行一些大运算量的计算是很艰难的。现在有了GPU，GPU用它的矩阵操作能力和大带宽为高计算量打开了新的大门。现在硬件技术还在继续发展，google开放的TPU据说比GPU还要快上25倍。总之无论如何，人们对大数据量技术的硬件技术探索不会停止。4、提到大数据，不得不提的就是大数据框架。目前比较主流的几个大数据框架包括hadoop、spark和storm。其中，hadoop可以说是最为经典也是最被人广泛使用的框架。它的前身是google提出的“三架马车”，GFS，MapReduce和Bigtable。现在hadoop已经成为了一个拥有完整生态系统的架构。然而hadoop也有它固有的缺点。比如，它的计算模式只有map和reduce两种，不够灵活；计算速度慢等。Spark的出现改善了这种情况，Spark采用内存计算机制，使计算更快，并设计了多种RDD，有多种操作。同时Spark生态系统中还综合了流处理、图计算等技术，使其成为了更加炙手可热的分布式框架。Storm主要用于流处理，近几年没那么火了，所以不说了。 2、机器学习技术机器学习是一系列算法的合称，这些算法均可以帮助我们从一群数据中挖掘出想要的信息。机器学习算法的分类有不同的指标，比较常见的是将机器学习算法分为监督学习、非监督学习和增强学习三个大类。1、监督学习：机器学习算法中最庞大的一个分支，拥有众多经典算法。监督学习需要训练数据，通过训练数据可以训练出一个模型。将新的数据放入训练出的模型后就能得到预测的结果。比较经典的监督学习算法包括决策树、支持向量机、朴素贝叶斯、逻辑斯蒂回归等。2、非监督学习，非监督学习不需要训练数据和训练过程，它利用已有数据自身的性质将它们天然地分成多类。常用的非监督学习主要是各种聚类算法，如K-means聚类、谱聚类、密度聚类、层次聚类等。3、增强学习：增强学习需要将训练的结果对训练过程做出反馈。如果训练结果是好的就给正反馈，如果训练结果不好就给负反馈。以此来不断调整学习的策略。 3、数据挖掘技术谈到数据挖掘技术，我主要想谈一谈数据分析技术。数据分析技术包括一个基本流程，这个流程如下所示：1、业务分析2、数据采集3、数据预处理4、数据建模5、模型评估6、结果可视化以上6步是一个完整的数据分析流程。其中，业务分析和数据采集是分析的准备阶段；数据预处理是数据分析的基础，它的目的是将采集回来的原始数据清洗、转换成为可以直接拿来建模的数据。例如，采回来的的数据中可能会有一些缺失数据、而且采集数据可能过于稀疏等，这些都需要进行变换。同时，特征工程也是数据预处理中至关重要的步骤。数据往往有诸多特征，这些特征也许并不是全都有用的。有很多冗余。有些特征甚至对数据还有负的影响作用。这些特征都需要特征工程去处理。数据建模就是将预处理好的数据用各种机器学习算法进行训练，得到训练模型。得到模型后，在模型评估阶段，我们将用测试数据对模型进行评估，常用的评估方法是k-折交叉验证法，常用的指标包括precision、recall、F-value等。最后的可视化主要是对结果的一个对外交互。属于比较上层的部分。","tags":[]},{"title":"","date":"2017-07-24T09:30:05.000Z","path":"2017/07/24/机器学习小白之路（序）———谈谈入坑和入坑后的感想/","text":"机器学习小白之路（序）———谈谈入坑和入坑后的感想一、我是如何入坑的我在读本科的时候对“机器学习”、“人工智能”等名词是完全没有概念的。当时读的是电子信息工程专业，每天主要就是和各种信号处理机制、滤波器、调制解调器等打交道，当时一心想读一个通信领域或信号领域的研究生，为国家的通讯事业奉献一生。然而一个偶然，读研期间选择了一个以“大数据”为主要研究方向的导师，自此生活便走上了与数据相关的道路，和信号处理那些东西就不在有什么关系了。记得我刚考上研究生的时候，对“大数据”毛线都不懂。然后就被导师要求写一篇关于大数据的综述调研。我硬着头皮鏖战了好几天，最终写出了一个以hadoop框架和spark框架为主的四不像的东西。最后导师看了估计心里暗暗叹了口气，然后这件事就没有然后了。但是在这次没什么技术含量的调研当中，我还是大致初窥了大数据领域的全貌。我大概了解到，大数据技术之所以在近期倍广泛的提出，主要是由于移动互联网和物联网的普及产生了大量数据，并且我们有了越来越强大的存储资源和计算资源。在有了数据，又有了处理数据的能力之后，我们便产生了从这些数据中挖掘信息的想法。因此也催生了大数据的一系列相关技术。大数据技术主要包括以下一些方面：大数据获取、大数据存储、大数据并行计算、大数据清洗处理、大数据中的数据挖掘、机器学习、结果可视化等。其中，大数据的数据挖掘和机器学习技术应该算是大数据各项技术中较为核心且更偏重于理论和算法等技术。它们也是从数据中挖掘信息的关键技术。可以说，无论是存储技术还是计算能力的提升，都是为通过机器学习等技术挖掘信息作准备工作的。因此，当时的我就对数据挖掘和机器学习技术有了较深的印象。比较严肃的扯了一堆，以上说的基本都是废话，真正使我入坑的原因是我是个臭美的妹子而在我苦恼淘宝找不到合适我的好看衣服时我猛然发现机器学习就是干这个的。于是我就入坑了，然后就没有然后了。 二、谈谈我的感想入坑后我就开始搜集各方面的资料，并寻找一些机会来学习机器学习的相关知识。当时主要是上了一些数据挖掘相关的课程，当时用的是R语言，跟着老师做了一些比较简单的手写数字识别、文理科分类、简单预测任务等。这个学科刚开始学的时候有一种越学越有趣的感觉。因为机器学习真的可以做好多有趣的事情，比如可以预测下一个季度的流行歌手，预测一场球赛的胜利、预测股市的涨跌等。虽然很多尝试仅仅可以用在实验中，比如预测股市的涨跌，如果你真的相信你训练的模型，搞不好会血本无归。但是这种憧憬是好的，学习机器学习的动力充足无比，好像学完了我就可以预测未来拯救地球一样。在简单学习了一些基础算法使用和R语言之后，我又开始学一些算法深层的东西，比如SVM的算法原理、决策树、朴素贝叶斯的算法原理等。从数学层面渐渐了解了更多的机器学习算法。学习的期间主要还看了一些书，比如李航的《统计学习方法》、周志华的《机器学习》等，这两本书写的真的很好，让我看到了所谓的大家风范。读书期间，慢慢领悟到，机器学习不仅仅是几个算法、几个任务，它更是一个需要综合多种领域知识的非常集大成的技术群。慢慢地也感受到了这个领域存在的竞争和难点。现在的我虽然还是一个小白，但对机器学习也有了一些更深的认识。最近也快要找工作了，发现大多数的机器学习岗位其实并不想一开始学习时想象的那么有趣。在工作中，可能有70%的时间是在“清洗数据”，也就是在乱七八糟数据中“捡垃圾”，挑出那些不能用的、对结果有负影响的数据；90%的时间在调参数，也就是把程序都写好了，然后用一个参数跑一遍，不好了换个参数再跑一遍。这些过程都是比较无聊的。那么高大上的算法部分去哪了呢？一般在程序中，那些著名的算法都被封装好了，用起来仅仅是一句话而已。以上就是我对机器学习领域对入坑到现在的一个整体认识。你说它神奇，它没那么神奇。你不可能学习它就去拯救地球，改变世界，成为下一个巴菲特，很多时候，你发现给你一堆数据让你分个男女你都只能得到70%左右的准确率；另一方面，你说它枯燥无聊，而它确实在慢慢改变这个世界。AlphaGo、无人车、增强现实等技术无一不是基于它的基础理论。因此对于机器学习，应该洗尽铅华，辩证的看待，毕竟它到底有没有用，还是要看你是不是大牛而已。","tags":[]}]